require('dotenv').config();
const express = require('express');
const multer = require('multer');
const OpenAI = require('openai');
const fs = require('fs');
const path = require('path');
const cors = require('cors');

const app = express();
const PORT = process.env.PORT || 3000;

// Configuration OpenAI
const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY
});

// Middlewares
app.use(cors());
app.use(express.json());
app.use(express.static('public'));

// Configuration du stockage pour les uploads
const storage = multer.diskStorage({
  destination: (req, file, cb) => {
    const uploadsDir = path.join(__dirname, 'uploads');
    if (!fs.existsSync(uploadsDir)) {
      fs.mkdirSync(uploadsDir, { recursive: true });
    }
    cb(null, uploadsDir);
  },
  filename: (req, file, cb) => {
    const uniqueName = `audio-${Date.now()}-${Math.round(Math.random() * 1E9)}${path.extname(file.originalname)}`;
    cb(null, uniqueName);
  }
});

const upload = multer({
  storage: storage,
  limits: {
    fileSize: 500 * 1024 * 1024 // 500 MB limite (pour 120 minutes d'audio)
  },
  fileFilter: (req, file, cb) => {
    const allowedTypes = /webm|mp3|wav|m4a|ogg|mp4/;
    const extname = allowedTypes.test(path.extname(file.originalname).toLowerCase());
    const mimetype = allowedTypes.test(file.mimetype);

    if (mimetype && extname) {
      return cb(null, true);
    } else {
      cb(new Error('Format audio non support√©. Formats accept√©s: webm, mp3, wav, m4a, ogg, mp4'));
    }
  }
});

// Route principale
app.get('/', (req, res) => {
  res.sendFile(path.join(__dirname, 'public', 'index.html'));
});

// Route pour transcrire et g√©n√©rer un rapport
app.post('/api/transcribe', upload.single('audio'), async (req, res) => {
  if (!req.file) {
    return res.status(400).json({ error: 'Aucun fichier audio fourni' });
  }

  if (!process.env.OPENAI_API_KEY) {
    return res.status(500).json({ error: 'Cl√© API OpenAI non configur√©e' });
  }

  const audioFilePath = req.file.path;

  try {
    console.log(`Transcription en cours pour: ${req.file.filename}`);

    // Transcription avec Whisper
    const transcription = await openai.audio.transcriptions.create({
      file: fs.createReadStream(audioFilePath),
      model: 'whisper-1',
      language: 'fr', // Fran√ßais par d√©faut
      response_format: 'verbose_json'
    });

    console.log('Transcription termin√©e');

    // G√©n√©ration du rapport sans afficher la transcription compl√®te
    const rapport = await genererRapport(transcription);

    // Supprimer le fichier audio apr√®s traitement
    fs.unlinkSync(audioFilePath);

    res.json({
      success: true,
      rapport: rapport,
      duree: transcription.duration,
      langue: transcription.language
    });

  } catch (error) {
    console.error('Erreur lors de la transcription:', error);

    // Nettoyer le fichier en cas d'erreur
    if (fs.existsSync(audioFilePath)) {
      fs.unlinkSync(audioFilePath);
    }

    res.status(500).json({
      error: 'Erreur lors de la transcription',
      details: error.message
    });
  }
});

// Fonction pour g√©n√©rer un rapport bas√© sur la transcription
async function genererRapport(transcription) {
  try {
    const prompt = `Tu es un assistant qui g√©n√®re des rapports structur√©s √† partir de transcriptions audio.
Voici une transcription d'un enregistrement audio :

"${transcription.text}"

G√©n√®re un rapport professionnel structur√© qui inclut :
1. Un r√©sum√© ex√©cutif (2-3 phrases)
2. Les points cl√©s abord√©s (liste √† puces)
3. Les actions ou d√©cisions importantes mentionn√©es
4. Une conclusion

Ne cite pas directement la transcription, reformule et structure l'information de mani√®re professionnelle.`;

    const completion = await openai.chat.completions.create({
      model: 'gpt-4o-mini',
      messages: [
        {
          role: 'system',
          content: 'Tu es un assistant professionnel qui g√©n√®re des rapports concis et bien structur√©s.'
        },
        {
          role: 'user',
          content: prompt
        }
      ],
      temperature: 0.7,
      max_tokens: 2000
    });

    return completion.choices[0].message.content;

  } catch (error) {
    console.error('Erreur lors de la g√©n√©ration du rapport:', error);
    // En cas d'erreur avec GPT, retourner un rapport basique
    return `# Rapport de Transcription

## R√©sum√©
Enregistrement audio transcrit avec succ√®s.

## Dur√©e
${Math.round(transcription.duration)} secondes (${Math.round(transcription.duration / 60)} minutes)

## Langue d√©tect√©e
${transcription.language}

## Note
Le contenu a √©t√© transcrit mais la g√©n√©ration automatique du rapport d√©taill√© a rencontr√© une erreur. Veuillez consulter la transcription compl√®te si n√©cessaire.`;
  }
}

// Route de sant√©
app.get('/api/health', (req, res) => {
  res.json({
    status: 'ok',
    openai_configured: !!process.env.OPENAI_API_KEY,
    max_duration_minutes: 120
  });
});

// D√©marrage du serveur
app.listen(PORT, () => {
  console.log(`üéôÔ∏è  Serveur Notivox lanc√© sur le port ${PORT}`);
  console.log(`üìù Acc√©dez √† l'application sur http://localhost:${PORT}`);
  console.log(`‚è±Ô∏è  Dur√©e maximale d'enregistrement: 120 minutes`);

  if (!process.env.OPENAI_API_KEY) {
    console.warn('‚ö†Ô∏è  ATTENTION: Cl√© API OpenAI non configur√©e!');
    console.warn('   Cr√©ez un fichier .env avec OPENAI_API_KEY=votre_cl√©');
  }
});
